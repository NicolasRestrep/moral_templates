---
title: "The role of template matching in the attribution of immorality"
author: "Nicolas Restrepo"
date: '`r format(Sys.time(), "%d %B, %Y")`'
fontsize: 12pt
output:
  pdf_document: default
  html_document: default
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{lineno}
  - \linenumbers
  - \def\linenumberfont{\normalfont\tiny\sffamily}
bibliography: template_references.bib
abstract: "While there has been progress in understanding moral decision-making, more research is needed to elucidate the cognitive processes that underpin the attribution of immorality. In this paper, I use sociological approaches to cultural meaning in order to test one of the leading theories of moral cognition: the idea that individuals attribute immorality through template matching. Analyzing the connotative structures of moral trangressions allows me to define what prototypicality means in the context of moral violations. Then, using reaction-time data, I examine whether deviations from such prototypicality inform how long it takes individuals to categorize scenarios as either immoral or harmful. Lastly, I test these results on a corpus of naturally occurring text in order to assess their external validity. These studies allow me to provide empirical evidence that supports the notion that the attribution of immorality occurs through template matching; a theory that, while increasingly popular, has remained largely untested."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.align='center')
```

```{r packages}
# Packages 
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggrepel)
library(gridExtra)
library(haven)
library(lavaan)
library(lme4)
library(nlme)
library(stargazer)
library(broom)
library(psych)
library(brms)
library(plotly)
library(sjPlot)
library(plot3D)
library(scatterplot3d)
library(quanteda)
library(brmstools)
library(bayesplot)
theme_set(theme_light())
```

```{r initial data setup}
# Load in the conservative data
c_data <- read_csv("Data/conservatives_full.csv")

# Get rid of the first 2 rows (they don't mean anything)
c_data <- c_data[-(1:2), ]

# Load pretest data 
p_data <- read_csv("data/pretest.csv")

# Same issue with the rows
p_data <- p_data[-(1:3), ]

# Load the liberal data 
l_data <- read_csv("data/liberals_complete.csv")

l_data <- l_data[-(1:2),]

# Now bind them together 
data <- rbind(c_data, p_data, l_data)

# Get attention checks for the data 
data <- data %>% 
  mutate(passed_1 = ifelse(ach_1_1 == "extremely harmful", 1, 0), 
         passed_2 = ifelse(ach_2_1 == "not at all immoral", 1, 0), 
         passed_3 = ifelse(ach_3_1 == "moderately unexpected", 1, 0), 
         passed_4 = ifelse(ach_4_1 == "not at all harmful", 1, 0), 
         passed_5 = ifelse(ach_5_1 == "extremely immoral", 1, 0), 
         passed_6 = ifelse(ach_6_1 == "extremely unexpected", 1, 0), 
         sum_pass = passed_1 + passed_2 + passed_3 + passed_4 + passed_5 + passed_6)

# Now drop cases which did not pass the attention checks 
data <- data %>% 
  filter(sum_pass == 6)

# Create variable that indicates if someone is conservative
data <- data %>% 
  mutate(conservative = ifelse(ideology_1 == 5 | ideology_1 == 6 | ideology_1 == 7, 1, 0)) 


# Mutate all main questions so that we get numbers 
data <- data %>% 
  mutate_at(vars(contains("_h_1")), funs(recode(., 
                                   "not at all harmful" = 1, 
                                   "slightly harmful" = 2, 
                                   "moderately harmful" = 3, 
                                   "very harmful" = 4, 
                                   "extremely harmful" = 5))) %>% 
  mutate_at(vars(contains("_i_1")), funs(recode(., 
                                   "not at all immoral" = 1, 
                                   "slightly immoral" = 2, 
                                   "moderately immoral" = 3, 
                                   "very immoral" = 4, 
                                   "extremely immoral" = 5))) %>% 
  mutate_at(vars(contains("_u_1")), funs(recode(., 
                                   "not at all unexpected" = 1, 
                                   "slightly unexpected" = 2, 
                                   "moderately unexpected" = 3, 
                                   "very unexpected" = 4, 
                                   "extremely unexpected" = 5)))


# Get of of the first set of uninformative columns 
d <- data %>% 
  select(-contains("ach")) %>% 
  select(pkp_h_1:msas_u_1, ideology_1, conservative)

# Now let's create a more amenable dataset 
d <- d %>% 
  mutate(id = 1:n()) %>% 
  select(id, everything()) 

# Rename all variables 
d <- d %>%  
  rename_all(
    funs(str_remove(., "_1"))
    ) %>% 
  mutate_if(is.character, as.numeric)


# Reshape harmful perceptions from wide to long 
d_harm_long <- d %>% 
  select(id, ideology, conservative, contains("_h")) %>% 
  gather(key = "scenario", value = "harm", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_h$"))

# Reshape immoral perceptions from wide to long 

d_imm_long <- d %>% 
  select(id, ideology, conservative, contains("_i")) %>% 
  gather(key = "scenario", value = "immoral", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_i$"))

# Reshape unexpected perceptions from wide to long 

d_un_long <- d %>% 
select(id, ideology, conservative, contains("_u")) %>% 
  gather(key = "scenario", value = "unexpected", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_u$"))

# Now join them 
d_long <- d_harm_long %>% 
  mutate(immoral = d_imm_long$immoral, 
         unexpected = d_un_long$unexpected)

# Load dataset with the deflections 
selected_events <- read_csv("Data/full_events.csv")

# Create column for abreviations 
events <- c("pch", 
            "pkp", 
            "tmdp", 
            "adr", 
            "elb", 
            "acr", 
            "jbd", 
            "scc", 
            "ddf", 
            "git", 
            "idb", 
            "ayc", 
            "sip", 
            "ecc", 
            "ccr", 
            "mbr", 
            "msn", 
            "mbw", 
            "php", 
            "mmc", 
            "mmls", 
            "msas", 
            "mpsa", 
            "whh", 
            "thls")

# Create new column 
events_def <- selected_events %>% 
  mutate(scenario = events) %>% 
  select(scenario, def, type, 5:33)

# Add to long dataset 
d_long <- d_long %>% 
  left_join(events_def, by = "scenario")

# create a summary table for the values of each question 
gd <- d %>% 
  select(-c(id, ideology, conservative)) %>% 
  gather(key = "scenario", 
         value = "score") %>% 
  group_by(scenario) %>% 
  summarise_all(funs(med = median(.), avg = mean(.), maximum = max(.), minimum = min(.), st_dv = sd(.), fq = quantile(., 0.25), tq = quantile(., 0.75))) 

gd <- gd %>% 
  mutate(scenario = str_replace(scenario, "acr", "athlete_cheats_rival"), 
         scenario = str_replace(scenario, "adr", "athlete_deceives_referee"), 
         scenario = str_replace(scenario, "ayc", "athlete_yells_at_coach"), 
         scenario = str_replace(scenario, "ccr", "coach_cheers_rival"), 
         scenario = str_replace(scenario, "ddf", "daughter_disobeys_father"),
         scenario = str_replace(scenario, "ecc", "employee_conspires_with_comp."),
         scenario = str_replace(scenario, "elb", "employee_lies_to_boss"), 
         scenario = str_replace(scenario, "git", "girl_interrupts_teacher"),
         scenario = str_replace(scenario, "idb", "intern_disobeys_boss"), 
         scenario = str_replace(scenario, "jbd", "judge_befriends_defendant"), 
         scenario = str_replace(scenario, "mbr", "man_betrays_relative"), 
         scenario = str_replace(scenario, "mbw", "man_betrays_wife"), 
         scenario = str_replace(scenario, "mmc", "man_marries_cousin"),
         scenario = str_replace(scenario, "mmls", "man_makes_love_sister"), 
         scenario = str_replace(scenario, "mpsa", "married_has_sex_adulterer"), 
         scenario = str_replace(scenario, "msas", "mother_sexually_arouses_son"), 
         scenario = str_replace(scenario, "msn", "mayor_slanders_neighbor"), 
         scenario = str_replace(scenario, "pch", "person_hurts_child"), 
         scenario = str_replace(scenario, "php", "person_hires_prostitute"), 
         scenario = str_replace(scenario, "pkp", "person_kills_person"),
         scenario = str_replace(scenario, "scc", "student_cheats_classmate"), 
         scenario = str_replace(scenario, "sip", "student_insults_professor"), 
         scenario = str_replace(scenario, "thls", "teacher_hits_lazy_student"), 
         scenario = str_replace(scenario, "whh", "wife_hits_husband"), 
         scenario = str_replace(scenario, "tmdp", "teenager_mocks_disabled"))

```
## Introduction

Although research on how individuals come to perceive actions as immoral has been disjointed, evidence from different fields suggests that actors are swift arbiters of right and wrong [@greenePointandShootMoralityWhy2014; @greeneRiseMoralCognition2015; @haidtIntuitiveEthicsHow2004]. There is less certainty, however, about the processes whereby we come to those instinctive judgments. At present, one of the most plausible explanations is that we attribute moral wrongdoing through template matching [@scheinTheoryDyadicMorality2018]. When we perceive an event, we compare it to our idea of what the typical moral transgression looks like. If the perceived event closely matches our cognitive template of an immoral act, then we are likely to view it as a transgression. Even though this theory has gained traction, evidence to support it remains limited. Here, I operationalize and test it. I take advantage of the fact that research on moral cognition has mainly used vignettes. Using Osgood et al.’s [-@osgoodCrossculturalUniversalsAffective1975] approach to measuring meaning, I produce “translations” of these vignettes into comparable units and examine their connotative structures. This helps me accomplish two goals: first, I identify shared semantic features among the scenarios in order to outline what a prototypical immoral act looks like; second, I calculate the extent to which the vignettes match this template, shedding light on how distance from the exemplar is related to the attribution of immorality. I examine the informativeness of this measure of distance in survey data and in naturally occurring text.
 
Perhaps the most widespread explanation of how individuals attribute immorality argues that there are qualitatively different types of moral transgressions, and that actors use distinct cognitive strategies to understand them. This explanation has emerged as a response to an empirical puzzle: the fact that there are events that individuals find immoral, but which do not involve harm [@haidtAffectCultureMorality1993]. The most prominent theory in this line of work is Moral Foundations Theory (henceforth MFT) [@grahamMoralFoundationsTheory2013]. MFT scholars argue that the diversity of moral judgments is explained by the fact that morality is underpinned by five foundations: authority, fairness, loyalty, purity, and harm. We attribute immorality differently depending on the moral foundation that appears to be breached. For instance, seeing a soccer player bend the rules to their favor is different that watching them deliberately try to injure a rival. Both are immoral acts but while the former evokes  sentiments about fairness, the latter involves concerns about harm. Variations in moral worldviews arise as individuals and groups emphasize certain foundations – or combinations of foundations – over others [@haidtMoralMindHow2007]. 

A more unified conceptualization of morality has regained traction throughout the last decade. Evidence showing that the five moral foundations are all highly correlated has been at the center of this theoretical push [@grayDisconfirmingMoralFoundations2015]. This suggests that transgressions cannot be parceled out into different groups. Gray and Schein [-@scheinTheoryDyadicMorality2018] contend that the unifying dimension of moral decision-making is harm. Their work shows that when individuals are asked to envision immoral acts, they tend to bring up quintessentially harmful practices, and that when asked to explain what makes certain actions immoral, they tend to resort to ideas of harmfulness [@scheinUnifyingMoralDyad2015; @scheinTheoryDyadicMorality2018]. Perceptions of harm even mediate our understanding of practices where no entities appear to be directly affected: incestual marriages, for instance, are often described as harming the health – the gene pool – of the group [@scheinTheoryDyadicMorality2018]. Their argument is that, regardless of the superficial differences between moral violations, the central mechanism through which individuals come to see them as immoral is harm. 

The theory of moral cognition that is congruent with this continuous vision of morality relies on the notion of template matching [@grayMindPerceptionEssence2012]. Template matching is a cognitive process whereby individuals categorize information by contrasting it with salient mental exemplars [@scheinTheoryDyadicMorality2018; @harnadCognizeCategorizeCognition2017]. Thus, category membership is not ascribed through inclusionary/exclusionary criteria but rather through proximity to a mental template. What the proponents of this position ague is that when we are asked to picture a moral transgression, we have a particular mental image of what that looks like [@scheinTheoryDyadicMorality2018]. When confronted with an event, we compare it with this cognitive template in order to assess its immorality. Moral cognition, then, involves placing a violation on a continuum, more or less distant from our mental template of a typical moral wrong.

Proponents of this theory provide a description of this mental template, but details remain opaque and empirical evidence remains limited. Immoral actions, they argue, have a dyadic structure: they involve an intentional agent directing a damaging behavior towards a vulnerable patient [@scheinTheoryDyadicMorality2018]. An event that would resemble this template would be a criminal murdering a child. When we witness an event, we compare it to this template and assess its immorality accordingly. Though plausible, this definition remains vague. It is unclear how we can define what constitutes a vulnerable victim or a damaging behavior without incurring in tautologies. Therefore, we do not know how to gauge whether an event fits into this template, let alone how to calculate how practices might deviate from it. Furthermore, we lack direct empirical tests of whether and how template matching mediates the attribution of immorality. My argument is that we can empirically study this process by analyzing the underlying connotative features of moral transgressions.

The central challenge is to outline a definition of the prototypical moral violation. An important step towards that goal is to realize that mental templates are eminently social [@hunzakerMappingCulturalSchemas2019; @hunzakerCulturalSentimentsSchemaConsistency2016a]. The cultural landscapes in which we are embedded shape how we envision certain categories, and the exemplars around which they are organized [@monkColorPunishmentAfrican2019; @hunzakerMappingCulturalSchemas2019; @martinPoliticalPositionSocial2010]. The key implication here is that we can treat the effort to outline a template of moral transgressions in the same way as scholars have outlined prototypes in other social categories (cf. [@bergstrandAdvantagedCauseAffect2019]). We can examine the underlying cultural meanings associated with immoral acts in order to look for regularities. There are two main challenges in this approach: first, we are dealing not with a single entity but with a dyadic structure; second, we need to find rigorous measures of cultural meaning that we can map onto these different components. Given fortuitous theoretical overlaps, it is possible to use the dictionaries of affective meanings collected by Affect Control Theory (henceforth ACT) scholars to address these challenges.

ACT scholars hold that cultural meaning is measurable and widely shared among members of the same social group [@heiseExpressiveOrderConfirming2007]. Building on Osgood et al.'s [-@osgoodCrossculturalUniversalsAffective1975] work, they further that the meaning of any concept can be effectively distilled to three dimensions: evaluation, potency, and activity. In other words, we need three pieces of information to broadly capture the cultural meaning of a concept: whether it is good or bad (evaluation), whether it is weak or strong (potency), and whether it is lively or quiet (activity) [@osgoodWhysWherefores1969]. Extensive cross-cultural work has shown that these three dimensions broadly capture connotative meanings, both across cultures and throughout time [@osgoodCrossculturalUniversalsAffective1975].

Using these dimensions, it is possible to collect rigorous measures of the meanings that groups attach to concepts. These aggregate meanings are collected by asking respondents to rate concepts on three, nine-point scales corresponding to each of the dimensions [@heiseCulturalVariationsSentiments2014]. These measures are then aggregated, and the average is computed, yielding the evaluation, potency, and activity (EPA) values for each concept. These EPA profiles are accurate and informative. Babies, for instance, are very good, very weak, and somewhat lively, while murderers are depicted as very bad, very powerful, and slightly lively. As part of their research agenda, ACT researchers have collected large dictionaries of affective meanings. I use the online ACT dictionary collected in 2015 to map reliable measures of cultural meaning onto the different components of transgressions [@robinsonMeanAffectiveRatings2016].

Research on moral cognition has primarily used vignettes to examine how actors assess immorality. In these studies, respondents are presented with hypothetical events and they must decide how immoral or harmful they are. Using the dictionaries of cultural meanings collected by ACT scholars, it is possible to translate these vignettes into a format that allows for the analysis of their connotative meanings. Let me provide an example. A scenario that has been shown [@cliffordMoralFoundationsVignettes2015] to be interpreted reliably is:

>You see a teacher hitting a student’s hand with a ruler for falling asleep in class.

Ascribing appropriate EPA profiles to each component of the sentence, we can produce the following translation:

>Teacher [E = 2.5; P = 2.31; A = 0.32] hits [E = -2.66; P = 1.30; A = 2.12] lazy student [E = -0.42; P = -0.76; A = -1.56]

Much like an x-ray, the translation allows us to examine the underlying structure of the event. This information, in turn, is useful in the effort to understand template matching in the context of moral cognition. For instance, it is possible to examine what semantic features play an important role in the attribution of immorality; we can look for identifiable attributes of typically harmful actions – such as murder – and prototypically vulnerable victims – such as children. 

Here, I use this approach to assess the prototypicality of moral transgressions and to analyze how deviations from those regularities shape moral decision-making. In Study 1, I analyze which semantic attributes are most informative for understanding the attribution of immorality. This, in turn, helps me reach a principled formalization of what the template of an immoral act looks like. In Study 2, I examine if an event’s distance from said template is predictive of how long it takes individuals to categorize it as immoral or harmful. In Study 3, I derive a hypothesis from the previous analyses and use natural language processing tools to test it in a corpus, comprising all Vox articles published before March 21st of 2017 (the data-set was compiled by Elena Zheleva, and is available [here](https://data.world/elenadata/vox-articles)).

## Results 

### Study 1

I translated 25 vignettes and had participants rate them based on their harmfulness, immorality, and unexpectedness. To compile the list of scenarios, I drew on the work of Gray and Keeney [-@grayImpureJustWeird2015], Clifford et. al. [-@cliffordMoralFoundationsVignettes2015], and Young and Saxe [-@youngWhenIgnoranceNo2011]. I chose scenarios that had been previously used and that I could translate using ACT’s dictionaries. I also decided to include five scenarios for each of the five moral foundations. The full list of scenarios are available in the supplemental materials. 

Each participant rated the vignettes using five-point Likert scales. I collected the data through the Prolific platform, which has advantages over other online experimental pools: it focuses on data quality and has a replenishing pool of participants [@peerTurkAlternativePlatforms2017]. In total, I reached out to 205 participants and, after removing cases with missed attention checks, the total number of usable responses was 194. 

Figure 1 shows the relationship between the average immorality of each scenario and its average harmfulness and average unexpectedness. The results reaffirm the close relationship between immorality and harm. Moreover, the positive relationship between immorality and unexpectedness resonates with Gray and Keeney's [-@grayImpureJustWeird2015] idea that severe transgressions tend to be perceived as more unexpected because they often entail the breaching of social mores. There are then encouraging patterns that are consistent with findings in the literature: the “harm” transgressions tend to be rated as the most severe, while the “purity” violations rank among the most unexpected. This suggests that there was not a significant loss of information when the vignettes were translated into the new format.


```{r}

# create mean measures from the long dataset

dsl <- d_long %>% 
  group_by(scenario, type, def, dop) %>% 
  summarise(mean_harm = mean(harm), 
            mean_imm = mean(immoral), 
            mean_unex = mean(unexpected)) 

dsl_gat <- dsl %>% 
  pivot_longer(cols = c("mean_harm", "mean_unex"), names_to = "dependent") %>% 
  mutate(dependent = if_else(dependent == "mean_harm", "harmfulness", "unexpectedness"))

dsl_gat %>% 
  ggplot(aes(x = mean_imm, y = value, shape = type)) + 
  geom_point(col = "darkred") + 
  facet_wrap(~dependent) + 
  theme(legend.position = "bottom") + 
  labs(x = "Immorality", 
       y = "", 
       title = "Figure 1") + 
  theme(plot.title = element_text(hjust = 0.5))
```

Figure 2 shows the coefficient plots for two multi-level models, where immorality rating is the dependent variable and the event’s semantic components are the independent variables. The models include varying intercepts for each participant and scenario. In the second model, I exclude the so-called “purity” scenarios from the analysis, following previous literature that has argued that these transgressions are distinctive, either because they are a different type of violation [@grahamMoralFoundationsTheory2013] or because they are unconventional [@grayImpureJustWeird2015]. Both models suggest that there are three semantic features that are particularly informative when considering the immorality of an event: the behavior’s evaluation, the behavior’s potency, and the object’s potency. Though there is uncertainty around some of these coefficients – especially in the first model - both analyses point at the informativeness of the same three semantic components. Reassuringly, these results are consistent with Gray and Schein’s theory. Negative and potent behaviors are perceived as more immoral. In turn, as the object’s potency increases, the immorality of an acts tends to decrease. This offers a more concrete definition of the notion of a “vulnerable patient”. As the recipient of an action appears less potent, said action is considered more immoral. 

```{r}
b1 <- readRDS(file ="fitted_models/ir_complete_cases.rds")
b2 <- readRDS(file = "fitted_models/ir_no_purity.rds")
post_b1 <- posterior_samples(b1) %>% 
  select(1:10) %>% 
  mutate(model = "Including Purity")
post_b2 <- posterior_samples(b2) %>% 
  select(1:10) %>% 
  mutate(model = "Excluding Purity")

all_post <- rbind(post_b1, post_b2)
names(all_post) <- c("Intercept", "Act. evaluation",
                     "Act. potency", 
                     "Act. activity", 
                     "Beh. evaluation", 
                     "Beh. potency", 
                     "Beh. activity", 
                     "Obj. evaluation", 
                     "Obj. potency", 
                     "Obj. activity", 
                     "model")

all_post %>% 
  pivot_longer(cols = 1:10) %>% 
  ggplot(aes(x = value, y = reorder(name, value))) +  
  geom_vline(xintercept = 0, color = "firebrick4", alpha = 1/10) + 
  geom_violin(fill = "firebrick4", 
              alpha = 0.8) + 
  facet_wrap(~model) + 
  labs(x = "Immorality Rating", 
       y = "", 
       title = "Figure 2") + 
  theme(plot.title = element_text(hjust = 0.5))

```

Establishing the importance of these three features allows me to place events in a three-dimensional space shown in Figure 3. This is key for two main reasons. First, it helps me reach a principled definition of a prototypical moral transgression: it should involve a potent and negative event directed at a very weak object. Using the terms that I have tested, the most appropriate event I can outline is: 

>A person kills a child. 

The second reason is that this allows me to calculate distance between the scenarios and the template defined above. This is the key piece of information needed to empirically test whether template matching mediates the attribution of immorality and/or harmfulness. I define distance from the template as the Euclidean distance between that point and a particular event in the three-dimensional space presented in Figure 3. Thus, if the prototypical transgression is $p$, then distance for scenario $i$ will be defined as: 

$$ D_i = \sqrt{(BP_p - BP_i)^2 + (BE_p - BE_i)^2 + (OP_p - OP_i)^2} $$

```{r}
events_def <- events_def %>% 
  mutate(type_num = case_when(type == "harm" ~ 1, 
                              type == "purity" ~ 2, 
                              type == "fairness" ~ 3, 
                              type == "authority" ~ 4, 
                              type == "loyalty" ~ 5)) %>% 
  mutate(type_num = as.factor(type_num))
levels(events_def$type_num) <- c("harm", "purity", "fairness", "authority", "loyalty")
shapes <-  c(5:9) 
shapes <- shapes[as.numeric(events_def$type_num)]

s3d <- scatterplot3d(events_def[, c(7,8,11)], pch = shapes, main = "Figure 3", xlab = "behavior evaluation", ylab = "behavior potency", 
                     zlab = "object potency", box = F, 
                     color = "darkred")

legend('top', legend = levels(events_def$type_num),
       pch = c(5:9), border = NULL, bty = "n", horiz = T, cex = 0.7, 
       col = "darkred")
```


### Study 2

These data were also collected through the Prolific platform. I collected data from a total of 200 participants, and after controlling for missed attention checks, there were a total 184 usable cases. In this study, I asked participants to categorize the same 25 scenarios as either immoral or not immoral, harmful or harmless. Using their keyboards, the participants chose the category to which they think the event belongs. The outcome variable of interest is the time it takes respondents to categorize the violations. Reaction time here is meant to stand as a proxy for cognitive effort [@grayImpureJustWeird2015; @mooreFastSlowSociological2017]. 

The main goal here is to examine the relationship between distance from the prototypical moral transgression and the time it takes individuals to categorize the scenarios. I examined this relationship using multi-level models with varying intercepts for participants and scenarios. The most informative model, in this case, is one that depicts a quadratic relationship between distance from the prototype and reaction time – the model selection criteria are available in the supplemental materials. This relationship follows similar patterns for the classification of immorality and harmfulness, but there is a lot of uncertainty around the coefficients for the model predicting the former (see Figure 4). This is certainly an important limitation but given the close interconnection between immorality and harm, the results remain insightful. Figure 5 depicts the quadratic relationship: events that are very close and very far from the prototype take lower time to classify. 

```{r}
# Load in conservative data
rt_conservative <- read_csv("Data/rt_conservative.csv")

# Load in liberal data 
rt_liberal <- read_csv("Data/rt_liberal.csv")

# Delete unnecessary rows 
rt_conservative <- rt_conservative[-(1:2),]
rt_liberal <- rt_liberal[-(1:2),]

# How many people passed the attention checks?
rt_conservative <- rt_conservative %>% 
  mutate(passed = if_else(att_1 == "Harmless (K)" & att_2 == "Immoral (J)" & att_3 == "Harmful (J)", 1, 0)) 

rt_liberal <- rt_liberal %>% 
  mutate(passed = if_else(att_1 == "Harmless (K)" & att_2 == "Immoral (J)" & att_3 == "Harmful (J)", 1, 0)) 


# Join both datasets 
data <- rbind(rt_liberal, rt_conservative)

# Filter people who did not pass
data <- data %>% 
  filter(passed == 1)

# Now let's turn this into a long dataframe 

# Create a manageable wide dataframe 

rtw <- data %>% 
  select(ends_with("Page Submit"), i_phc, i_pkp, i_tmdp, i_adr, i_elb, 
         i_acr, i_jbd, i_scc, i_ddf, i_git, i_idb, i_ayc, i_sip, i_ecc, 
         i_ccr, i_mbr, i_msn, i_mbw, i_php, i_mmc, i_mmls, i_msas, i_mpsa, i_whh, i_thls, h_phc, h_pkp, h_tmdp, h_adr, h_elb, 
         h_acr, h_jbd, h_scc, h_ddf, h_git, h_idb, h_ayc, h_sip, h_ecc, 
         h_ccr, h_mbr, h_msn, h_mbw, h_php, h_mmc, h_mmls, h_msas, h_mpsa, h_whh, h_thls) %>% 
  mutate(ids = 1:184) %>% 
  select(ids, everything())


# Create a long dataframe for harm and reaction time

d_long_harm_rt <- rtw %>% 
  select(ids, starts_with("h_")) %>% 
  pivot_longer(2:26, names_to = "scenario", values_to = "reaction_time_harm") %>% 
  select(ids, scenario, reaction_time_harm) %>% 
  mutate(scenario = str_remove(scenario, "_t_Page Submit$")) %>% 
  mutate(scenario = str_remove(scenario, "^h_"))

# Create a long dataframe for is_harm

d_long_harm_bin <- rtw %>% 
  select(ids, starts_with("h_")) %>% 
  pivot_longer(27:51, names_to = "scenario", values_to = "is_harmful") %>% 
  select(ids, scenario, is_harmful) %>% 
  mutate(scenario = str_remove(scenario, "^h_"))

d_long_harm <- left_join(d_long_harm_rt, d_long_harm_bin, by = c('ids', "scenario"))

# Create a long dataframe for immorality and reaction time 

d_long_imm_rt <- rtw %>% 
  select(ids, starts_with("i_")) %>% 
  pivot_longer(2:26, names_to = "scenario", values_to = "reaction_time_imm") %>% 
  select(ids, scenario, reaction_time_imm) %>% 
  mutate(scenario = str_remove(scenario, "_t_Page Submit$")) %>% 
  mutate(scenario = str_remove(scenario, "^i_"))

# Create a long dataframe for immorality and is_immoral

d_long_imm_bin <- rtw %>% 
  select(ids, starts_with("i_")) %>% 
  pivot_longer(27:51, names_to = "scenario", values_to = "is_immoral") %>% 
  select(ids, scenario, is_immoral) %>% 
  mutate(scenario = str_remove(scenario, "^i_"))

d_long_imm <- left_join(d_long_imm_rt, d_long_imm_bin, by = c('ids', 'scenario'))

# Now join them 

d_long <- left_join(d_long_harm, d_long_imm, by = c('ids', 'scenario')) %>% 
  mutate_at(c('reaction_time_harm', 'reaction_time_imm'), as.numeric)


# Let's get some information about the events in there 

# Load dataset with the deflections 

selected_events <- read_csv("data/full_events.csv")

# Create column for abreviations 

events <- c("phc", 
            "pkp", 
            "tmdp", 
            "adr", 
            "elb", 
            "acr", 
            "jbd", 
            "scc", 
            "ddf", 
            "git", 
            "idb", 
            "ayc", 
            "sip", 
            "ecc", 
            "ccr", 
            "mbr", 
            "msn", 
            "mbw", 
            "php", 
            "mmc", 
            "mmls", 
            "msas", 
            "mpsa", 
            "whh", 
            "thls")

# Create new column 

events_def <- selected_events %>% 
  mutate(scenario = events) %>% 
  select(scenario, def, type, 5:33)

# Add to long dataset 

d_long <- d_long %>% 
  left_join(events_def, by = "scenario")

# How many weird responses do we have? 
# There are 66 responses that don't seem valid. Let's delete them

d_long <- d_long %>% 
  filter(reaction_time_harm >= 1 & 
           reaction_time_imm >= 1 & 
           reaction_time_harm <= 10 & 
           reaction_time_imm <= 10)
# Recode variables to binary 

d_long <- d_long %>%  
  mutate(is_immoral = case_when(is_immoral == "Immoral (J)" ~ 1, 
                                is_immoral == "Not Immoral (K)" ~ 0), 
         is_harmful = case_when(is_harmful == "Harmful (J)" ~ 1, 
                                is_harmful == "Harmless (K)" ~ 0))

# Create a new variable for the variances 

d_long_var <- d_long %>% 
  group_by(scenario) %>% 
  summarise(p_harm = mean(is_harmful, na.rm = T), 
            p_imm = mean(is_immoral, na.rm = T), 
            v_harm = var(is_harmful, na.rm = T), 
            v_imm = var(is_immoral, na.rm = T))

# Join back to the long dataframe 

d_long <- left_join(d_long, d_long_var, by = "scenario")

dsl <- d_long %>% 
  group_by(scenario, type) %>% 
  summarise(mean_harm = mean(reaction_time_harm), 
            mean_imm = mean(reaction_time_imm)) %>% 
  ungroup()

# Name the scenarios with the full length 

dsl <- dsl %>% 
  mutate(full_string = case_when(scenario == "acr" ~ "An athlete cheats their rival", 
                                 scenario == "adr" ~ "An athlete deceives the referee", 
                                 scenario == "ayc" ~ "An athlete yells at their coach", 
                                 scenario == "ccr" ~ "A coach cheers for the rival", 
                                 scenario == "ddf" ~ "A daughter disobeys her father", 
                                 scenario == "ecc" ~ "An employee conspires with a competitor", 
                                 scenario == "elb" ~ "An employee lies to the boss", 
                                 scenario == "git" ~ "A girl interrupts her teacher", 
                                 scenario == "idb" ~ "An intern disobeys their boss", 
                                 scenario == "jbd" ~ "A judge befriends the defendant", 
                                 scenario == "mbr" ~ "A man betrays his relative", 
                                 scenario == "mbw" ~ "A man betrays his wife", 
                                 scenario == "mmc" ~ "A man marries his cousin", 
                                 scenario == "mmls" ~ "A man makes love to his sister", 
                                 scenario == "mpsa" ~ "A married person has sex with an adulterer", 
                                 scenario == "msas" ~ "A mother sexually arouses her son", 
                                 scenario == "msn" ~ "A mayor slanders a neighbor", 
                                 scenario == "phc" ~ "A person hurts a child", 
                                 scenario == "php" ~ "A person hires a prostitute", 
                                 scenario == "pkp" ~ "A person kills a person", 
                                 scenario == "scc" ~ "A student cheats their classmate", 
                                 scenario == "sip" ~ "A student insults the professor", 
                                 scenario == "thls" ~ "A teacher hits a lazy student", 
                                 scenario == "whh" ~ "A wife hits her forgetful husband", 
                                 scenario == "tmdp" ~ "A teenager mocks a disabled person"))

# Add the short versions again for the plot

dsl <- dsl %>% 
  mutate(length = str_count(full_string)) 

# Calculate readibility 

readibility_indices <- c(NA, rep = 25)

for (i in 1: 25) {
  
  y <- textstat_readability(dsl$full_string[i], measure = "Flesch.Kincaid")
  readibility_indices[i] <- y$Flesch.Kincaid
}

# Join with dsl 

dsl <- cbind(dsl, readibility_indices)

# Create function to calculate the euclidean distance in three-dimensional space 

events_values <- events_def %>% 
  select(scenario, op, be, bp)

eucl_dist_pkp <- function(x) { 
  op <-  events_values[x, 2] 
  be <-  events_values[x, 3] 
  bp <-  events_values[x, 4]
  
  # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt((0.95 - op)^2 + ((-4.26) - be)^2 + (1.95 - bp)^2)
  return(as.double(distance))
}


distances_pkp <- map_dbl(1:25, eucl_dist_pkp)

events_values_pkp <- events_values %>% 
  cbind(distances_pkp) %>% 
  select(scenario, distances_pkp)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_pkp, by = "scenario")

# Create new function with Person hurts child as the prototype 

eucl_dist_phc <- function(x) { 
  op <-  events_values[x, 2] 
  be <-  events_values[x, 3] 
  bp <-  events_values[x, 4]
  
  # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt(((-1.14) - op)^2 + ((-3.17) - be)^2 + (1.06 - bp)^2)
  return(as.double(distance))
}


distances_phc <- map_dbl(1:25, eucl_dist_phc)

events_values_phc <- events_values %>% 
  cbind(distances_phc) %>% 
  select(scenario, distances_phc)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_phc, by = "scenario")

# Create new function with Person hurts child as the prototype 

eucl_dist_prot <- function(x) { 
  op <-  events_values[x, 2] 
  be <-  events_values[x, 3] 
  bp <-  events_values[x, 4]
  
  # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt(((-1.14) - op)^2 + ((-4.26) - be)^2 + (1.95 - bp)^2)
  return(as.double(distance))
}


distances_prot <- map_dbl(1:25, eucl_dist_prot)

events_values_prot <- events_values %>% 
  cbind(distances_prot) %>% 
  select(scenario, distances_prot)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_prot, by = "scenario")

lengths <- dsl %>% 
  select(scenario, length, readibility_indices, distances_phc, distances_pkp, distances_prot)

d_long <- d_long %>% 
  select(1:16, 36:39) %>% 
  left_join(lengths, by = "scenario")

# Add one column for the average reaction time of the person 

# Create the averages 
avg_rt <- d_long %>% 
  select(ids, reaction_time_harm, reaction_time_imm) %>% 
  pivot_longer(2:3) %>% 
  group_by(ids) %>% 
  summarise(avg_rt = mean(value))

# Add averages to long data 

d_long <- left_join(d_long, avg_rt, by = "ids")

# Add ideological beliefs

data_pol <- data %>% 
  select(ideology_1) %>% 
  mutate(ids = 1:184)

d_long <- left_join(d_long, data_pol, by = "ids")

# Scale variables 

d_long_scaled <- d_long %>% 
  mutate_at(c("ids", "type", "scenario", "is_harmful", "is_immoral"), ~as.factor(.)) %>% 
  mutate_if(is.numeric, scale)

b3 <- readRDS(file ="fitted_models/rt_harm_model.rds")
b4 <- readRDS(file = "fitted_models/rt_imm_model.rds")
post_b3 <- posterior_samples(b3) %>% 
  select(1:5) %>% 
  mutate(model = "Harm")
post_b4 <- posterior_samples(b4) %>% 
  select(1:5) %>% 
  mutate(model = "Immorality")

all_post <- rbind(post_b3, post_b4)

names(all_post) <- c("Intercept", 
                     "Distance",
                     "Distance Sqrd.", 
                     "Length", 
                     "Readibility", 
                     "Model")

all_post %>% 
  pivot_longer(cols = 1:5) %>% 
  ggplot(aes(x = value, y = reorder(name, value))) +  
  geom_vline(xintercept = 0, color = "firebrick4", alpha = 1/10) + 
  geom_violin(fill = "firebrick4", 
              alpha = 0.8) + 
  facet_wrap(~Model) + 
  labs(x = "Reaction Time", 
       y = "", 
       title = "Figure 4") + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
marg <- marginal_effects(b3)
new_data <- marg$distances_prot
ggplot() + 
  geom_jitter(data = d_long_scaled, aes(x=distances_prot, y=reaction_time_harm, col = type),alpha=0.2) + 
  geom_line(data = new_data, aes(x = distances_prot, y = estimate__), size = 1.3, color = "blue") + 
  geom_ribbon(data = new_data, aes(x = distances_prot, y = estimate__, ymax = upper__, ymin = lower__), fill="skyblue4",alpha=0.3) + 
  labs(x = "Distance from the Prototype (std)", 
       y = "Reaction Time", 
       title = "Reaction Time by Distance from Prototype", 
       subtitle = "Raw data and fitted model",
       caption = "Figure 5") 
```

An important concern is that most of the events that are very distant from the prototype are the so-called purity scenarios. Study 1 shows that these scenarios are consistently ranked among the most harmful. Thus, it is possible that these transgressions are driving the quadratic effect. Nonetheless, I ran the same models taking these scenarios out of the sample and the quadratic relationship still held; these results are available in the supplemental materials. I also fit a logistic regression predicting whether an event was classified as harmful using distance from the prototype as the main independent variable. As Figure 6 shows, holding the length and readability of the vignettes constant, increases in distance from the prototype lead to a lower predicted probability of an event being classified as harmful. The so-called purity scenarios then are "out of place": their connotative structures do not seem to correspond to the harmfulness that participants attribute to them. 

```{r}
b5 <- readRDS(file = "fitted_models/log_model_harmful.rds")

p <- marginal_effects(b5, effects = "distances_prot") 

new_data <- p$distances_prot

ggplot() + 
  geom_line(data = new_data, aes(x = distances_prot, y = estimate__), size = 1.3, color = "darkred") + 
  geom_ribbon(data = new_data, aes(x = distances_prot, y = estimate__, ymax = upper__, ymin = lower__), fill="red",alpha=0.3) + 
  labs(x = "Distance from template (std)", 
       y = "Probability harmful", 
       title = "Is an event harmful?", 
       caption = "Figure 6\nShaded region is the 95% Confidence Interval") + 
  theme(plot.title = element_text(hjust = 0.5)) 
```

### Study 3 

The above studies share an important limitation: the events considered have similar semantic structures, and they were all curated to represent well-established moral violations. To rectify this, I examine whether distance from the template is a useful piece of information when analyzing a wide set of events extracted from naturally occurring text. 

I choose to analyze whether distance from the moral template is informative of the kind of language co-occurring around events. Following the discussion above, it is possible to hypothesize that events that are close to the exemplar are more likely to be surrounded by words that would betray moral transgression. Thus, I test the following hypothesis:

> The rate of negative moral words around an event should be negatively related to its distance from the moral template. 

For this analysis, I use a data-set consisting of all articles published by the news site Vox before the 21st of March 2017. This data-set was collected by Elena Zhelenko and made public through the social platform data.world. In total, the corpus contains 23022 articles published between 2014 and 2017. These data prove particularly valuable for the problem at hand because Vox covers a wide array of topics, which ensures that I am able to capture a relatively wide set of events. To extract these events, I use the Python libraries spaCy and textacy, which offer automated parts-of-speech recognition tools. As a concrete example, take this sentence from one of the articles:

> By the end of September 2013, the Obama administration had deported 1.83 million immigrants.

After extracting the subject-verb-object triplet, we are left with: 

> The Obama Administration had deported immigrants. 

This sentence has the same structures as the vignettes used in the previous studies. This means we can map the EPA values collected by Affect Control Theorists to the extracted triplets, provided that the definitions of their components are in the existent dictionaries. In total there are 9969 triplets, spanning 6354 articles, for which the necessary EPA scores are available. 

To operationalize the notion of "negative moral vocabulary", I use the Moral Foundations Dictionary. This dictionary was compiled by Graham and Haidt and provides a list of words for that are meant to capture the positive and negative aspects of each foundation, as well as a section of "general morality" terms. For this analysis, I use a list consisting only of the negative words; a total of 165 terms.

My hypothesis posits a negative relationship between how distant an event is from the moral template and the amount of negative moral vocabulary around it. In order to build the dependent variable, I extract excerpts of text around each event and calculate the number of words that coincide with the negative terms from the Moral Foundations Dictionary. The excerpts consist of the fifty words prior to the first term of the triplet and the fifty words following it. The length of the window here represents a trade-off. A narrow window might not be enough to capture all of the moral vocabulary used around the events, but casting a wide net might result in too many false positives. I conducted the same analysis below using windows of 50, 25 and 10 words - available in the supplemental materials - without appreciable differences in the results. 

Figure 7 shows the predicted count of negative moral words when holding length of the excerpt and overall sentiment at their medians. As hypothesized, there is a negative relationship.  An event that is two standard deviations closer than average to the template has a predicted count of about 1.2 negative moral words. In contrast, a triplet that is two standard deviations further away than average is predicted to have around 0.6 negative moral terms surrounding it. Although the effect size is relatively small, this finding lends credence to the idea that distance from the moral template is useful for understanding the moral language that co-occurs around an event. This is especially apparent when we consider that distance from the template remains informative even when we have taken general sentiment into account. The connotative structures of the events are not only predictive of whether an event is presented in a generally positive or negative light. Rather they give us additional information about the moral language that is being used around the triplets. 

```{r}
b6 <- readRDS(file = "fitted_models/poiss_model.rds")

p <- marginal_effects(b6, effects = "dist_stand") 

new_data <- p$dist_stand

ggplot() + 
  geom_line(data = new_data, aes(x = dist_stand, y = estimate__), size = 1.3, color = "darkred") + 
  geom_ribbon(data = new_data, aes(x = dist_stand, y = estimate__, ymax = upper__, ymin = lower__), fill="red",alpha=0.3) + 
  labs(x = "Distance from template", 
       y = "Count", 
       title = "Predicted count of negative moral words", 
       caption = "Figure 7\nShaded region is the 95% Confidence Interval") + 
  theme(plot.title = element_text(hjust = 0.5)) 
```

## Discussion

This project provides a novel technique for analyzing moral transgressions. The majority of work on moral cognition has used hypothetical scenarios to elicit moral judgments from participants [@cliffordIndividualDifferencesGroup2017].  We can use the dictionaries of semantic profiles collected by ACT scholars to translate vignettes into formats that allow us to explore their connotative structures. Doing so yields insightful findings. For instance, Study 1 shows that, echoing Gray and Schein’s [-@scheinTheoryDyadicMorality2018] arguments, transgressions have shared regularities: they involve a negative and potent behavior directed towards a weak object. We can bring rigor to their definition of an immoral template: a damaging behavior could be considered an action which is low on evaluation but high on potency; a vulnerable patient could be described as an actor who has a low potency score. 

By establishing connotative regularities of moral transgressions, I outline a formal definition of what the template of an immoral act looks like and to calculate deviations from it. Then I directly test the proposition that moral cognition occurs through template matching. The results from Study 2 show that distance from the template is informative of how long it takes individuals to classify events as harmful. Interestingly, the results from Study 2 also suggest that the relationship is quadratic: as distance increases reaction time increases, but only up to a point. Individuals are quick to classify events that are very close or far apart from the template. Events that lie at a middling distance are more ambiguous and take longer to be classified. These findings, then, provide empirical evidence for the notion that template matching mediates moral cognition, and also give a clearer idea of how this process occurs. 

Study 3 shows that these ideas are informative when considering a wider set of events, extracted from naturally occurring text. Using tools from computational natural language processing, it is possible to extract subject-verb-object triplets from text corpora. These triplets, in turn, have the same grammatical structure as the template I constructed, which allows for their "translation". The results from Study 3  provide evidence for the external validity of the survey findings. The ability to map connotative meanings to naturally occurring text opens up the possibility of testing a host of hypotheses about morality. Here, I show that by looking at the connotative structures of triplets, it is possible to gain knowledge about the negative moral vocabulary that occurs around them. The possibilities increase further when we consider that there is an active strand of research that attempts - quite successfully - to use machine learning techniques to derive EPA scores for terms that are not in the collected dictionaries [@loonCanWeDistill2019]. This would allow us to explore the connotative structures underpinning moral judgments in a much wider range of settings. 

These studies also shed light on an important debate within the study of moral cognition. Many of the theoretical developments in the study of morality have been animated by the question of whether there are such things as harmless wrongs [@tepeHarmfulnessImpurityMoral2019]. The results from Study 1 side with Gray and Schein [-@scheinTheoryDyadicMorality2018], noting that some of the so-called “purity” transgressions are perceived as harmful. Yet, the findings from Study 2 compel us to rethink the distinctiveness of these scenarios. These violations do not match the patterns that the models uncover in the data; they are distant from the template and very harmful. Both sides of the debate, then, have pointed out important aspects of this type of transgressions. Gray and Schein [-@scheinTheoryDyadicMorality2018] are right in insisting that that purity scenarios are understood through the lens of harm. However, these transgressions **are** distinctive insofar as their semantic structures do not match their perceived harmfulness. My findings suggest that we should move away from the debate about “harmless” wrongs, but they compel us to consider how practices that are distant from the moral template come to be regarded as harmful [@scheinMoralizationHarmificationDyadic2016]. 

## Conclusion

These studies represent a concrete test of one of the most plausible contemporary theories of moral cognition. Current evidence supports a more unified vision of moral judgment, where perceptions of harm are the main drivers of this process. Gray and Schein have outlined a model of moral cognition that is consistent with this vision, where immorality is ascribed through template matching. The evidence to support this position, however, remains limited. Here, I device a technique for directly examining this theory. I use existent repositories of cultural meanings to examine the connotative structures of moral transgressions. This allows me to accomplish two things: to produce a rigorous definition of a prototypical moral wrong, and to examine whether deviations from it affect how individuals perceive transgressions. The results suggest that distance from the prototypical moral transgression is informative of the time it takes individuals to classify scenarios as harmful. Furthermore, distance from the template is also predictive of the negative moral vocabulary around events in naturally occurring text. 

## Methods 

### Study 1 

Each participant was presented with the 25 translated scenarios and asked to rate how immoral, harmful, and unexpected they were. They provided their ratings using five-point Likert scales that went from, for example, “not harmful at all” to “extremely harmful”. The decision to include these three scales was based on previous work, which had examined these dimensions [@grayImpureJustWeird2015; @scheinTheoryDyadicMorality2018]. 

For the models presented in the results, I fit cross-classified, multi-level models predicting immorality ratings on the event’s semantic components. These models include varying intercepts for individuals and scenarios in order to account for systematic differences between events and respondents. The models are defined as follows: 

$$I_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \alpha_{scenario_i} + \alpha_{participant_i} + \beta_1Ae + \beta_2Ap +\beta_3Aa + ... + \beta_9Oa$$

### Study 2 

In this study, participants classify the scenarios using their keyboards. For each question, the scenario appeared in the middle of the screen and the dichotomous categories were shown at the bottom, on opposite sides of the page. Each participant had to classify all scenarios on two dimensions and, therefore, I have a total of 9200 recorded times. To ensure that I could capture the initial reactions as reliably as possible, I designed the questionnaire so that participants had a ten-second limit to classify each scenario (with a one-second pause between questions). Out of all the responses, only 44 were above 10 seconds and 22 were below 1 second. Given that these data most likely reflect misclicks or technical difficulties, I decided to delete these cases. 

The main goal of this analysis is to examine the relationship between distance from the prototypical moral transgression and the time it takes individuals to categorize the scenarios. As argued above, I define distance from the template as follows: 

$$ D_i = \sqrt{(BP_p - BP_i)^2 + (BE_p - BE_i)^2 + (OP_p - OP_i)^2} $$

Here,  I fit cross-classified, multi-level models where the main independent variable is distance from the template and the outcome variable is reaction time. They include varying intercepts for participants and scenarios and controls for length and readability. Reaction time data can be fickle and the length and complexity of scenarios certainly play a role in how long it takes respondents to categorize them. The models, then, include a term for the number of characters in each event and another coefficient with their respective Flesch-Kincaid readability score – an established measure of the level of education necessary to understand a text. They are defined as follows: 

$$RT_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \alpha_{scenario_i} + \alpha_{participant_i} + \beta_1Distance + \beta_2Distance^2 +\beta_3Length + \beta_4Readibility$$
The logistic regression model shown in Figure 6 includes the same controls; the outcome variable here is whether an event was classified as harmful and the main explanatory variable is again distance from the template. It follows the form: 

$$Harmful_i \sim Binomial(n_i = 1, p_i)$$
$$logit(p_i) = \alpha + \alpha_{scenario_i} + \alpha_{participant_i} + \beta_1Distance +\beta_3Length + \beta_4Readibility$$

### Study 3 

My hypothesis posits a negative relationship between how distant an event is from the moral template and the amount of negative moral vocabulary that occurs around it. In order to build the dependent variable, I extract excerpts of text around each event and calculate the number of words that coincide with the negative terms from the Moral Foundations Dictionary. Before checking for matches, I lemmatize and stem the words in both the excerpts and the dictionary. This helps identify overlaps, even if modified versions or alternative spellings of the words are used. In this analysis, the outcome variable represents a count and, therefore, a Poisson regression is the most appropriate. Thus, I regress the count of matches on the event's distance from the template, controlling for the length of the excerpt and the overall sentiment of the text, which I calculate using the NLTK library's sentiment analyzer (VADER). The model is specified as follows: 

$$Count_i \sim Poisson(\lambda_i)$$
$$log(\lambda_i) = \alpha + \beta_1Distance + \beta_2Length + \beta_3Sentiment$$

### Limitations 

An important limitation of Studies 1 and 2 concerns the number and type of fictional vignettes used. Given that I only use 25 vignettes, I rely on limited variation to produce my estimates. Furthermore, the events are all practices that are relatively immoral. Effectively what I am doing is a form of selecting on the dependent variable. Study 3 tries to ameliorate this by testing the findings on a  wider set of events, showing the external validity of the results. However, I arrived at the hypothesis using a limited sample of events. Future work should focus on exploring events with widely varying connotative structures.

Another potential shortcoming of this analysis is related to the sensitivity of one of the instruments. Reaction time studies are fickle and here I am adding complexity to this task. I am asking participants to react to full sentences and to do so remotely. Though the data show consistent patterns, these limitations should be taken seriously. In future work, it would be useful to find ways of measuring moral cognition more precisely.

Lastly, Study 3 shows the possibilities that natural language processing offers to the study of morality but these techniques are prone to error. For instance, the parts-of-speech tools I implement to extract the triplets cannot recognize sentences with complex structures. Moreover, tools like sentiment analysis are correctly identified as blunt, incapable of discerning important context cues like irony. Despite these shortcomings, the results provide important insights, showing that researchers can use naturally occurring text to probe at interesting questions in the study of morality.

## References: 

