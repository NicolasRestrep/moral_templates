{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598297513854",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the SVOs \n",
    "\n",
    "Here, I am going to calculate the sentiment around the SVO triplets for which I already have EPA values. There is going to be a lot of work here. I need to: \n",
    "\n",
    "1) Retrieve the whole document.\n",
    "3) Create a moving window around the triplet. \n",
    "4) Extract that text. \n",
    "5) Conduct sentiment analysis on that text. \n",
    "\n",
    "Let's begin by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, VERB\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to import the two dataframes: 1) The full text articles 2) the triplets for which I already have scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full articles\n",
    "vox = pd.read_csv(\"C:/Users/nro04/Documents/moral_templates/Data/vox_articles.csv\")\n",
    "# Drop NAs before continuing with the analysis \n",
    "# Remember we did this in the original analysis\n",
    "vox = vox.dropna(subset=['clean_strings'])\n",
    "# Import known SVOs \n",
    "already_known = pd.read_csv(\"C:/Users/nro04/Documents/moral_templates/Data/already_known.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out these datasets to make sure everything is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   index               subject             verb      object  start   end  \\\n0     34                  that     might entice   consumers   1176  1182   \n1     35            generation           expose   consumers   1224  1243   \n2    102          legalization       encourages     doctors   1320  1336   \n3    360           white house  having deported  immigrants      0    12   \n4    373  obama administration     had deported  immigrants    342   356   \n\n  subj_dep subj_tag obj_dep obj_tag  Document clean_verbs clean_objs  \n0    nsubj       DT    dobj     NNS         0      entice   consumer  \n1    nsubj       NN    dobj     NNS         0      expose   consumer  \n2    nsubj       NN    dobj     NNS         1   encourage     doctor  \n3    nsubj      NNP    dobj     NNS         7      deport  immigrant  \n4    nsubj       NN    dobj     NNS         7      deport  immigrant  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>subject</th>\n      <th>verb</th>\n      <th>object</th>\n      <th>start</th>\n      <th>end</th>\n      <th>subj_dep</th>\n      <th>subj_tag</th>\n      <th>obj_dep</th>\n      <th>obj_tag</th>\n      <th>Document</th>\n      <th>clean_verbs</th>\n      <th>clean_objs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>that</td>\n      <td>might entice</td>\n      <td>consumers</td>\n      <td>1176</td>\n      <td>1182</td>\n      <td>nsubj</td>\n      <td>DT</td>\n      <td>dobj</td>\n      <td>NNS</td>\n      <td>0</td>\n      <td>entice</td>\n      <td>consumer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35</td>\n      <td>generation</td>\n      <td>expose</td>\n      <td>consumers</td>\n      <td>1224</td>\n      <td>1243</td>\n      <td>nsubj</td>\n      <td>NN</td>\n      <td>dobj</td>\n      <td>NNS</td>\n      <td>0</td>\n      <td>expose</td>\n      <td>consumer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>legalization</td>\n      <td>encourages</td>\n      <td>doctors</td>\n      <td>1320</td>\n      <td>1336</td>\n      <td>nsubj</td>\n      <td>NN</td>\n      <td>dobj</td>\n      <td>NNS</td>\n      <td>1</td>\n      <td>encourage</td>\n      <td>doctor</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>360</td>\n      <td>white house</td>\n      <td>having deported</td>\n      <td>immigrants</td>\n      <td>0</td>\n      <td>12</td>\n      <td>nsubj</td>\n      <td>NNP</td>\n      <td>dobj</td>\n      <td>NNS</td>\n      <td>7</td>\n      <td>deport</td>\n      <td>immigrant</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>373</td>\n      <td>obama administration</td>\n      <td>had deported</td>\n      <td>immigrants</td>\n      <td>342</td>\n      <td>356</td>\n      <td>nsubj</td>\n      <td>NN</td>\n      <td>dobj</td>\n      <td>NNS</td>\n      <td>7</td>\n      <td>deport</td>\n      <td>immigrant</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Check already known dataframe\n",
    "already_known.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks okay. \n",
    "\n",
    "I am going to write a function that is be able to identify the triplet's document and extract the text around it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    return(string)\n",
    "\n",
    "def extract_sentiment(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    sent_score = sid.polarity_scores(string)\n",
    "    return(sent_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function across all triplets, taking 50 words before and after the event. \n",
    "\n",
    "This takes a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working on row 0\nworking on row 100\nworking on row 200\nworking on row 300\nworking on row 400\nworking on row 500\nworking on row 600\nworking on row 700\nworking on row 800\nworking on row 900\nworking on row 1000\nworking on row 1100\nworking on row 1200\nworking on row 1300\nworking on row 1400\nworking on row 1500\nworking on row 1600\nworking on row 1700\nworking on row 1800\nworking on row 1900\nworking on row 2000\nworking on row 2100\nworking on row 2200\nworking on row 2300\nworking on row 2400\nworking on row 2500\nworking on row 2600\nworking on row 2700\nworking on row 2800\nworking on row 2900\nworking on row 3000\nworking on row 3100\nworking on row 3200\nworking on row 3300\nworking on row 3400\nworking on row 3500\nworking on row 3600\nworking on row 3700\nworking on row 3800\nworking on row 3900\nworking on row 4000\nworking on row 4100\nworking on row 4200\nworking on row 4300\nworking on row 4400\nworking on row 4500\nworking on row 4600\nworking on row 4700\nworking on row 4800\nworking on row 4900\nworking on row 5000\nworking on row 5100\nworking on row 5200\nworking on row 5300\nworking on row 5400\nworking on row 5500\nworking on row 5600\nworking on row 5700\nworking on row 5800\nworking on row 5900\nworking on row 6000\nworking on row 6100\nworking on row 6200\nworking on row 6300\nworking on row 6400\nworking on row 6500\nworking on row 6600\nworking on row 6700\nworking on row 6800\nworking on row 6900\nworking on row 7000\nworking on row 7100\nworking on row 7200\nworking on row 7300\nworking on row 7400\nworking on row 7500\nworking on row 7600\nworking on row 7700\nworking on row 7800\nworking on row 7900\nworking on row 8000\nworking on row 8100\nworking on row 8200\nworking on row 8300\nworking on row 8400\nworking on row 8500\nworking on row 8600\nworking on row 8700\nworking on row 8800\nworking on row 8900\nworking on row 9000\nworking on row 9100\nworking on row 9200\nworking on row 9300\nworking on row 9400\nworking on row 9500\nworking on row 9600\nworking on row 9700\nworking on row 9800\nworking on row 9900\nworking on row 10000\nworking on row 10100\nworking on row 10200\n"
    }
   ],
   "source": [
    "list_sentiments = []\n",
    "\n",
    "for x in range(len(already_known)):\n",
    "    sent = extract_sentiment(row = x, padding = 50)\n",
    "    if (x % 100 == 0):\n",
    "        print(f'working on row {x}')\n",
    "    list_sentiments.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a series of lists so we can create a neat dataset we can later add to our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_list = []\n",
    "neutral_list = []\n",
    "positive_list = []\n",
    "compound_list = []\n",
    "\n",
    "for sents in list_sentiments: \n",
    "    neg = sents['neg']\n",
    "    neu = sents['neu']\n",
    "    pos = sents['pos']\n",
    "    com = sents['compound']\n",
    "    negative_list.append(neg)\n",
    "    neutral_list.append(neu)\n",
    "    positive_list.append(pos)\n",
    "    compound_list.append(com) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn lists into a dictionary and then data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dictionary = {'negative': negative_list, \n",
    "                       'neutral': neutral_list, \n",
    "                       'positive': positive_list, \n",
    "                       'compound': compound_list}\n",
    "sent_df = pd.DataFrame(sent_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new dataframe to our existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([already_known, sent_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally save our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/Users/nro04/Documents/moral_templates/Data/known_triplets_sentiments.csv\")"
   ]
  }
 ]
}