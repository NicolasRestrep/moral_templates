{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599840557288",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count of moral words \n",
    "\n",
    "Here, I am going to try to compile a moral dicitionary and count occurrences of words within it  in the fragments of text around the SVOs.\n",
    "\n",
    "I begin by loading the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import string \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, VERB\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the dictionary. I start from the 'general morality' section in the MFT dictionary (https://moralfoundations.org/wp-content/uploads/files/downloads/moral%20foundations%20dictionary.dic) and look for synonyms to cast a wider net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moral_dictionary = [\"harm\", \"suffer\", \"war\", \"warlord\", \"fight\", \"violent\", \"hurt\", \"kill\", \"killer\", \"endanger\", \n",
    "\"cruel\", \"brutal\", \"abuse\", \"damage\", \"ruin\", \"ravage\", \"detriment\", \"crush\", \n",
    "\"attack\", \"annihilate\", \"destroy\", \"stomp\", \"abandon\", \"spurn\", \"impair\", \"exploit\", \"wound\", \"unfair\", \"unequal\", \"bias\", \"unjust\", \"injustice\", \"bigot\", \"discriminate\", \"disproportionate\", \"inequitable\", \"prejudice\", \"dishonest\", \"unscrupulous\", \"dissociate\", \"preference\",\"favoritism\", \"segregate\", \"exclusion\", \"exclude\", \"foreign\", \"enemy\", \"betray\", \"treason\", \"traitor\", \"treachery\", \"disloyal\", \"individual\", \"apostasy\", \"apostate\", \"deserted\", \"deserter\", \"deceive\", \"jilt\", \"imposter\", \"miscreant\", \"spy\", \"sequester\", \"renegade\", \"terrorism\", \"immigration\", \"defiant\", \"rebel\", \"dissent\", \"subversive\", \"disrespect\", \"disobey\", \"agitator\", \"insubordinate\", \"illegal\", \"lawless\", \"insurgent\", \"mutinous\", \"defy\", \"dissident\", \"unfaithful\", \"alienate\", \"defector\", \"heretic\", \"nonconformist\", \"oppose\", \"protest\", \"refuse\", \"denounce\", \"remonstrate\", \"riot\", \"obstruct\", \"disgust\", \"deprave\", \"disease\", \"unclean\", \"contagion\", \"indecent\", \"sin\", \"sinful\", \"sinner\", \"sinned\", \"slut\", \"whore\", \"dirty\", \"impiety\", \"impious\", \"profane\", \"gross\", \"repulsive\", \"sick\", \"promiscuous\", \"lewd\", \"adulterer\", \"debaucherie\", \"defile\", \"tramp\", \"prostitute\", \"unchaste\", \"intemperate\", \"wanton\", \"profligate\", \"filth\", \"trashy\", \"obscene\", \"lax\", \"taint\", \"stain\", \"tarnish\", \"debase\", \"desecrate\", \"wicked\",\"blemish\", \"exploitation\", \"pervert\", \"wretched\", \"righteous\", \"moral\", \"ethic\", \"value\", \"upstanding\", \"good\", \"goodness\", \"principle\", \"blameless\", \"exemplary\", \"lesson\", \"canon\", \"doctrine\", \"noble\", \"worth\", \"ideal\", \"praiseworthy\", \"commendable\", \"character\", \"proper\", \"laudable\", \"correct\", \"wrong\", \"evil\", \"immoral\", \"bad\", \"offend\", \"offensive\", \"transgress\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem these words to get the roots. This will make finding modifications easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_moral = []\n",
    "\n",
    "for word in moral_dictionary:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    stemmed_moral.append(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I managed to construct the dictionary. Now, I need to check whether these words (or similar) occur around the triplets of interest. \n",
    "\n",
    "Let's import the dataframes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full articles\n",
    "vox = pd.read_csv(\"~/Documents/moral_templates/Data/vox_articles.csv\")\n",
    "# Drop NAs before continuing with the analysis \n",
    "vox = vox.dropna(subset=['clean_strings'])\n",
    "# Import known SVOs \n",
    "already_known = pd.read_csv(\"~/Documents/moral_templates/Data/known_triplets_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting text and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"the Drug Enforcement Agency should relax its classification for marijuana, which is currently marked as more dangerous than cocaine, to support more research. The problem isn't limited to just epilepsy, however. It's long been difficult to study marijuana due to federal restrictions. While this list is limited to six examples, it could certainly grow as legalization encourages doctors and researchers to take another look at a drug once marked as taboo.\""
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Just the extract text function for now \n",
    "def extract_text(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    return(string, start, end)\n",
    "\n",
    "text = extract_text(2, padding=50)\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['the', 'Drug', 'Enforcement', 'Agency', 'should', 'relax', 'its', 'classification', 'for', 'marijuana', 'which', 'is', 'currently', 'marked', 'as', 'more', 'dangerous', 'than', 'cocaine', 'to', 'support', 'more', 'research', 'The', 'problem', 'isnt', 'limited', 'to', 'just', 'epilepsy', 'however', 'Its', 'long', 'been', 'difficult', 'to', 'study', 'marijuana', 'due', 'to', 'federal', 'restrictions', 'While', 'this', 'list', 'is', 'limited', 'to', 'six', 'examples', 'it', 'could', 'certainly', 'grow', 'as', 'legalization', 'encourages', 'doctors', 'and', 'researchers', 'to', 'take', 'another', 'look', 'at', 'a', 'drug', 'once', 'marked', 'as', 'taboo']\n"
    }
   ],
   "source": [
    "res = re.sub('['+string.punctuation+']', '',text[0]).split()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working fine.\n",
    "\n",
    "Now, let's stem the fragment and look for coincidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_stemmed = []\n",
    "\n",
    "for word in res: \n",
    "    sw = stemmer.stem(word)\n",
    "    res_stemmed.append(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(set(stemmed_moral).intersection(res_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three coincidences above. \n",
    "\n",
    "Let's write a function that can do this iteratively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_moral_vocab(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    excerpt = f\"{text}\"\n",
    "    excerpt_split = re.sub('['+string.punctuation+']', '',excerpt).split()\n",
    "    ss_stemmed = []\n",
    "    for word in excerpt_split: \n",
    "        sw = stemmer.stem(word)\n",
    "        ss_stemmed.append(sw)\n",
    "    return (len(set(stemmed_moral).intersection(ss_stemmed)), start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all the rows of the already known triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working on row 0\nworking on row 100\nworking on row 200\nworking on row 300\nworking on row 400\nworking on row 500\nworking on row 600\nworking on row 700\nworking on row 800\nworking on row 900\nworking on row 1000\nworking on row 1100\nworking on row 1200\nworking on row 1300\nworking on row 1400\nworking on row 1500\nworking on row 1600\nworking on row 1700\nworking on row 1800\nworking on row 1900\nworking on row 2000\nworking on row 2100\nworking on row 2200\nworking on row 2300\nworking on row 2400\nworking on row 2500\nworking on row 2600\nworking on row 2700\nworking on row 2800\nworking on row 2900\nworking on row 3000\nworking on row 3100\nworking on row 3200\nworking on row 3300\nworking on row 3400\nworking on row 3500\nworking on row 3600\nworking on row 3700\nworking on row 3800\nworking on row 3900\nworking on row 4000\nworking on row 4100\nworking on row 4200\nworking on row 4300\nworking on row 4400\nworking on row 4500\nworking on row 4600\nworking on row 4700\nworking on row 4800\nworking on row 4900\nworking on row 5000\nworking on row 5100\nworking on row 5200\nworking on row 5300\nworking on row 5400\nworking on row 5500\nworking on row 5600\nworking on row 5700\nworking on row 5800\nworking on row 5900\nworking on row 6000\nworking on row 6100\nworking on row 6200\nworking on row 6300\nworking on row 6400\nworking on row 6500\nworking on row 6600\nworking on row 6700\nworking on row 6800\nworking on row 6900\nworking on row 7000\nworking on row 7100\nworking on row 7200\nworking on row 7300\nworking on row 7400\nworking on row 7500\nworking on row 7600\nworking on row 7700\nworking on row 7800\nworking on row 7900\nworking on row 8000\nworking on row 8100\nworking on row 8200\nworking on row 8300\nworking on row 8400\nworking on row 8500\nworking on row 8600\nworking on row 8700\nworking on row 8800\nworking on row 8900\nworking on row 9000\nworking on row 9100\nworking on row 9200\nworking on row 9300\nworking on row 9400\nworking on row 9500\nworking on row 9600\nworking on row 9700\nworking on row 9800\nworking on row 9900\nworking on row 10000\nworking on row 10100\nworking on row 10200\n"
    }
   ],
   "source": [
    "list_moral_vocab = []\n",
    "\n",
    "for x in range(len(already_known)):\n",
    "    cnt = count_moral_vocab(row = x, padding = 50)\n",
    "    if (x % 100 == 0):\n",
    "        print(f'working on row {x}')\n",
    "    list_moral_vocab.append(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column of moral count to the 'already known' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_known['moral_count'] = [i[0] for i in list_moral_vocab]\n",
    "\n",
    "already_known['excerpt_start'] = [i[1] for i in list_moral_vocab]\n",
    "\n",
    "already_known['excerpt_end'] = [i[2] for i in list_moral_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_known.to_csv('~/Documents/moral_templates/Data/triplets_mfd.csv')"
   ]
  }
 ]
}