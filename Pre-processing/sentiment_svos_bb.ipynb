{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the SVOs \n",
    "\n",
    "Here, I am going to calculate the sentiment around the SVO triplets for which I already have EPA values. There is going to be a lot of work here. I need to: \n",
    "\n",
    "1) Retrieve the whole document.\n",
    "3) Create a moving window around the triplet. \n",
    "4) Extract that text. \n",
    "5) Conduct sentiment analysis on that text. \n",
    "\n",
    "Let's begin by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, VERB\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to import the two dataframes: 1) The full text articles 2) the triplets for which I already have scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full articles\n",
    "vox = pd.read_csv(\"~/Documents/moral_templates/Data/breitbart_articles.csv\")\n",
    "# Drop NAs before continuing with the analysis \n",
    "# Remember we did this in the original analysis\n",
    "vox = vox.dropna(subset=['clean_strings'])\n",
    "# Import known SVOs \n",
    "already_known = pd.read_csv(\"~/Documents/moral_templates/Data/bb_already_known.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out these datasets to make sure everything is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>subj_dep</th>\n",
       "      <th>subj_tag</th>\n",
       "      <th>obj_dep</th>\n",
       "      <th>obj_tag</th>\n",
       "      <th>Document</th>\n",
       "      <th>publication</th>\n",
       "      <th>clean_verbs</th>\n",
       "      <th>clean_objs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>group</td>\n",
       "      <td>is urging</td>\n",
       "      <td>americans</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>NNPS</td>\n",
       "      <td>6</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>urge</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>landrieu</td>\n",
       "      <td>derided</td>\n",
       "      <td>critics</td>\n",
       "      <td>193</td>\n",
       "      <td>202</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NNP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>NNS</td>\n",
       "      <td>10</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>deride</td>\n",
       "      <td>critic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191</td>\n",
       "      <td>city</td>\n",
       "      <td>serves</td>\n",
       "      <td>tourists</td>\n",
       "      <td>658</td>\n",
       "      <td>664</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>NNS</td>\n",
       "      <td>10</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>serve</td>\n",
       "      <td>tourist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249</td>\n",
       "      <td>julia hahn</td>\n",
       "      <td>has followed</td>\n",
       "      <td>boss</td>\n",
       "      <td>177</td>\n",
       "      <td>209</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NNP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>NN</td>\n",
       "      <td>14</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>follow</td>\n",
       "      <td>boss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261</td>\n",
       "      <td>faith</td>\n",
       "      <td>has inspired</td>\n",
       "      <td>men</td>\n",
       "      <td>54</td>\n",
       "      <td>61</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>NNS</td>\n",
       "      <td>16</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>inspire</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     subject          verb     object  start  end subj_dep subj_tag  \\\n",
       "0     63       group     is urging  americans      0    7    nsubj       NN   \n",
       "1    175    landrieu       derided    critics    193  202    nsubj      NNP   \n",
       "2    191        city        serves   tourists    658  664    nsubj       NN   \n",
       "3    249  julia hahn  has followed       boss    177  209    nsubj      NNP   \n",
       "4    261       faith  has inspired        men     54   61    nsubj       NN   \n",
       "\n",
       "  obj_dep obj_tag  Document publication clean_verbs clean_objs  \n",
       "0    dobj    NNPS         6   Breitbart        urge   american  \n",
       "1    dobj     NNS        10   Breitbart      deride     critic  \n",
       "2    dobj     NNS        10   Breitbart       serve    tourist  \n",
       "3    dobj      NN        14   Breitbart      follow       boss  \n",
       "4    dobj     NNS        16   Breitbart     inspire        man  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check already known dataframe\n",
    "already_known.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks okay. \n",
    "\n",
    "I am going to write a function that is be able to identify the triplet's document and extract the text around it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    return(string)\n",
    "\n",
    "def extract_sentiment(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    sent_score = sid.polarity_scores(string)\n",
    "    return(sent_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function across all triplets, taking 50 words before and after the event. \n",
    "\n",
    "This takes a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on row 0\n",
      "working on row 100\n",
      "working on row 200\n",
      "working on row 300\n",
      "working on row 400\n",
      "working on row 500\n",
      "working on row 600\n",
      "working on row 700\n",
      "working on row 800\n",
      "working on row 900\n",
      "working on row 1000\n",
      "working on row 1100\n",
      "working on row 1200\n",
      "working on row 1300\n",
      "working on row 1400\n",
      "working on row 1500\n",
      "working on row 1600\n",
      "working on row 1700\n",
      "working on row 1800\n",
      "working on row 1900\n",
      "working on row 2000\n",
      "working on row 2100\n",
      "working on row 2200\n",
      "working on row 2300\n",
      "working on row 2400\n",
      "working on row 2500\n",
      "working on row 2600\n",
      "working on row 2700\n",
      "working on row 2800\n",
      "working on row 2900\n",
      "working on row 3000\n",
      "working on row 3100\n",
      "working on row 3200\n",
      "working on row 3300\n",
      "working on row 3400\n",
      "working on row 3500\n",
      "working on row 3600\n",
      "working on row 3700\n",
      "working on row 3800\n",
      "working on row 3900\n",
      "working on row 4000\n",
      "working on row 4100\n",
      "working on row 4200\n",
      "working on row 4300\n",
      "working on row 4400\n",
      "working on row 4500\n",
      "working on row 4600\n",
      "working on row 4700\n",
      "working on row 4800\n",
      "working on row 4900\n",
      "working on row 5000\n",
      "working on row 5100\n",
      "working on row 5200\n",
      "working on row 5300\n",
      "working on row 5400\n",
      "working on row 5500\n",
      "working on row 5600\n",
      "working on row 5700\n",
      "working on row 5800\n",
      "working on row 5900\n",
      "working on row 6000\n",
      "working on row 6100\n",
      "working on row 6200\n",
      "working on row 6300\n",
      "working on row 6400\n",
      "working on row 6500\n",
      "working on row 6600\n",
      "working on row 6700\n",
      "working on row 6800\n",
      "working on row 6900\n",
      "working on row 7000\n",
      "working on row 7100\n",
      "working on row 7200\n",
      "working on row 7300\n",
      "working on row 7400\n",
      "working on row 7500\n",
      "working on row 7600\n",
      "working on row 7700\n",
      "working on row 7800\n"
     ]
    }
   ],
   "source": [
    "list_sentiments = []\n",
    "\n",
    "for x in range(len(already_known)):\n",
    "    sent = extract_sentiment(row = x, padding = 50)\n",
    "    if (x % 100 == 0):\n",
    "        print(f'working on row {x}')\n",
    "    list_sentiments.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a series of lists so we can create a neat dataset we can later add to our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_list = []\n",
    "neutral_list = []\n",
    "positive_list = []\n",
    "compound_list = []\n",
    "\n",
    "for sents in list_sentiments: \n",
    "    neg = sents['neg']\n",
    "    neu = sents['neu']\n",
    "    pos = sents['pos']\n",
    "    com = sents['compound']\n",
    "    negative_list.append(neg)\n",
    "    neutral_list.append(neu)\n",
    "    positive_list.append(pos)\n",
    "    compound_list.append(com) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn lists into a dictionary and then data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dictionary = {'negative': negative_list, \n",
    "                       'neutral': neutral_list, \n",
    "                       'positive': positive_list, \n",
    "                       'compound': compound_list}\n",
    "sent_df = pd.DataFrame(sent_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new dataframe to our existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([already_known, sent_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally save our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"~/Documents/moral_templates/Data/known_triplets_sentiments_bb.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2ec54d4c82c0643e2c3695b3253674f992d325a3b7643f9e3cb9c951694bd1e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('text_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}