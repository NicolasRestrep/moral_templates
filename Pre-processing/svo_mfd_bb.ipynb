{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count of moral words \n",
    "\n",
    "Here, I am going to try to compile a moral dicitionary and count occurrences of words within it  in the fragments of text around the SVOs.\n",
    "\n",
    "I begin by loading the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import string \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, VERB\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the dictionary. I start from the 'general morality' section in the MFT dictionary (https://moralfoundations.org/wp-content/uploads/files/downloads/moral%20foundations%20dictionary.dic) and look for synonyms to cast a wider net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moral_dictionary = [\"harm\", \"suffer\", \"war\", \"warlord\", \"fight\", \"violent\", \"hurt\", \"kill\", \"killer\", \"endanger\", \n",
    "\"cruel\", \"brutal\", \"abuse\", \"damage\", \"ruin\", \"ravage\", \"detriment\", \"crush\", \n",
    "\"attack\", \"annihilate\", \"destroy\", \"stomp\", \"abandon\", \"spurn\", \"impair\", \"exploit\", \"wound\", \"unfair\", \"unequal\", \"bias\", \"unjust\", \"injustice\", \"bigot\", \"discriminate\", \"disproportionate\", \"inequitable\", \"prejudice\", \"dishonest\", \"unscrupulous\", \"dissociate\", \"preference\",\"favoritism\", \"segregate\", \"exclusion\", \"exclude\", \"foreign\", \"enemy\", \"betray\", \"treason\", \"traitor\", \"treachery\", \"disloyal\", \"individual\", \"apostasy\", \"apostate\", \"deserted\", \"deserter\", \"deceive\", \"jilt\", \"imposter\", \"miscreant\", \"spy\", \"sequester\", \"renegade\", \"terrorism\", \"immigration\", \"defiant\", \"rebel\", \"dissent\", \"subversive\", \"disrespect\", \"disobey\", \"agitator\", \"insubordinate\", \"illegal\", \"lawless\", \"insurgent\", \"mutinous\", \"defy\", \"dissident\", \"unfaithful\", \"alienate\", \"defector\", \"heretic\", \"nonconformist\", \"oppose\", \"protest\", \"refuse\", \"denounce\", \"remonstrate\", \"riot\", \"obstruct\", \"disgust\", \"deprave\", \"disease\", \"unclean\", \"contagion\", \"indecent\", \"sin\", \"sinful\", \"sinner\", \"sinned\", \"slut\", \"whore\", \"dirty\", \"impiety\", \"impious\", \"profane\", \"gross\", \"repulsive\", \"sick\", \"promiscuous\", \"lewd\", \"adulterer\", \"debaucherie\", \"defile\", \"tramp\", \"prostitute\", \"unchaste\", \"intemperate\", \"wanton\", \"profligate\", \"filth\", \"trashy\", \"obscene\", \"lax\", \"taint\", \"stain\", \"tarnish\", \"debase\", \"desecrate\", \"wicked\",\"blemish\", \"exploitation\", \"pervert\", \"wretched\", \"righteous\", \"moral\", \"ethic\", \"value\", \"upstanding\", \"good\", \"goodness\", \"principle\", \"blameless\", \"exemplary\", \"lesson\", \"canon\", \"doctrine\", \"noble\", \"worth\", \"ideal\", \"praiseworthy\", \"commendable\", \"character\", \"proper\", \"laudable\", \"correct\", \"wrong\", \"evil\", \"immoral\", \"bad\", \"offend\", \"offensive\", \"transgress\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem these words to get the roots. This will make finding modifications easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_moral = []\n",
    "\n",
    "for word in moral_dictionary:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    stemmed_moral.append(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I managed to construct the dictionary. Now, I need to check whether these words (or similar) occur around the triplets of interest. \n",
    "\n",
    "Let's import the dataframes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full articles\n",
    "vox = pd.read_csv(\"~/Documents/moral_templates/Data/breitbart_articles.csv\")\n",
    "# Drop NAs before continuing with the analysis \n",
    "vox = vox.dropna(subset=['clean_strings'])\n",
    "# Import known SVOs \n",
    "already_known = pd.read_csv(\"~/Documents/moral_templates/Data/known_triplets_sentiments_bb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for extracting text and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landrieu claimed. “The great migration that sent some of our best and brightest to places across the country that we don’t have the benefit of has been incredible. ” Historians have refuted this claim in 2015, when Landrieu first demanded the monuments come down. They argued that the city serves tourists and locals as a public history museum, unlike any other city in the U. S.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just the extract text function for now \n",
    "def extract_text(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    string = f\"{text}\"\n",
    "    return(string, start, end)\n",
    "\n",
    "text = extract_text(2, padding=50)\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Landrieu', 'claimed', '“The', 'great', 'migration', 'that', 'sent', 'some', 'of', 'our', 'best', 'and', 'brightest', 'to', 'places', 'across', 'the', 'country', 'that', 'we', 'don’t', 'have', 'the', 'benefit', 'of', 'has', 'been', 'incredible', '”', 'Historians', 'have', 'refuted', 'this', 'claim', 'in', '2015', 'when', 'Landrieu', 'first', 'demanded', 'the', 'monuments', 'come', 'down', 'They', 'argued', 'that', 'the', 'city', 'serves', 'tourists', 'and', 'locals', 'as', 'a', 'public', 'history', 'museum', 'unlike', 'any', 'other', 'city', 'in', 'the', 'U', 'S']\n"
     ]
    }
   ],
   "source": [
    "res = re.sub('['+string.punctuation+']', '',text[0]).split()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working fine.\n",
    "\n",
    "Now, let's stem the fragment and look for coincidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_stemmed = []\n",
    "\n",
    "for word in res: \n",
    "    sw = stemmer.stem(word)\n",
    "    res_stemmed.append(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stemmed_moral).intersection(res_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three coincidences above. \n",
    "\n",
    "Let's write a function that can do this iteratively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_moral_vocab(row, padding):\n",
    "    doc = nlp(vox.iloc[already_known.iloc[row]['Document']]['clean_strings'])\n",
    "    if already_known.iloc[row]['start']-padding <= 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = already_known.iloc[row]['start']-padding\n",
    "    if already_known.iloc[row]['end']+padding+2 >= len(doc):\n",
    "        end = len(doc)\n",
    "    else: \n",
    "        end = already_known.iloc[row]['end']+padding+2\n",
    "    text = doc[start:end]\n",
    "    excerpt = f\"{text}\"\n",
    "    excerpt_split = re.sub('['+string.punctuation+']', '',excerpt).split()\n",
    "    ss_stemmed = []\n",
    "    for word in excerpt_split: \n",
    "        sw = stemmer.stem(word)\n",
    "        ss_stemmed.append(sw)\n",
    "    return (len(set(stemmed_moral).intersection(ss_stemmed)), start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all the rows of the already known triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on row 0\n",
      "working on row 100\n",
      "working on row 200\n",
      "working on row 300\n",
      "working on row 400\n",
      "working on row 500\n",
      "working on row 600\n",
      "working on row 700\n",
      "working on row 800\n",
      "working on row 900\n",
      "working on row 1000\n",
      "working on row 1100\n",
      "working on row 1200\n",
      "working on row 1300\n",
      "working on row 1400\n",
      "working on row 1500\n",
      "working on row 1600\n",
      "working on row 1700\n",
      "working on row 1800\n",
      "working on row 1900\n",
      "working on row 2000\n",
      "working on row 2100\n",
      "working on row 2200\n",
      "working on row 2300\n",
      "working on row 2400\n",
      "working on row 2500\n",
      "working on row 2600\n",
      "working on row 2700\n",
      "working on row 2800\n",
      "working on row 2900\n",
      "working on row 3000\n",
      "working on row 3100\n",
      "working on row 3200\n",
      "working on row 3300\n",
      "working on row 3400\n",
      "working on row 3500\n",
      "working on row 3600\n",
      "working on row 3700\n",
      "working on row 3800\n",
      "working on row 3900\n",
      "working on row 4000\n",
      "working on row 4100\n",
      "working on row 4200\n",
      "working on row 4300\n",
      "working on row 4400\n",
      "working on row 4500\n",
      "working on row 4600\n",
      "working on row 4700\n",
      "working on row 4800\n",
      "working on row 4900\n",
      "working on row 5000\n",
      "working on row 5100\n",
      "working on row 5200\n",
      "working on row 5300\n",
      "working on row 5400\n",
      "working on row 5500\n",
      "working on row 5600\n",
      "working on row 5700\n",
      "working on row 5800\n",
      "working on row 5900\n",
      "working on row 6000\n",
      "working on row 6100\n",
      "working on row 6200\n",
      "working on row 6300\n",
      "working on row 6400\n",
      "working on row 6500\n",
      "working on row 6600\n",
      "working on row 6700\n",
      "working on row 6800\n",
      "working on row 6900\n",
      "working on row 7000\n",
      "working on row 7100\n",
      "working on row 7200\n",
      "working on row 7300\n",
      "working on row 7400\n",
      "working on row 7500\n",
      "working on row 7600\n",
      "working on row 7700\n",
      "working on row 7800\n"
     ]
    }
   ],
   "source": [
    "list_moral_vocab = []\n",
    "\n",
    "for x in range(len(already_known)):\n",
    "    cnt = count_moral_vocab(row = x, padding = 50)\n",
    "    if (x % 100 == 0):\n",
    "        print(f'working on row {x}')\n",
    "    list_moral_vocab.append(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column of moral count to the 'already known' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_known['moral_count'] = [i[0] for i in list_moral_vocab]\n",
    "\n",
    "already_known['excerpt_start'] = [i[1] for i in list_moral_vocab]\n",
    "\n",
    "already_known['excerpt_end'] = [i[2] for i in list_moral_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_known.to_csv('~/Documents/moral_templates/Data/triplets_mfd_bb.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2ec54d4c82c0643e2c3695b3253674f992d325a3b7643f9e3cb9c951694bd1e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('text_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}